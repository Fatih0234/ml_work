# **Gradient Descent Types: Batch, Stochastic, and Mini-Batch**

This repository demonstrates the three main types of **Gradient Descent**â€”**Batch Gradient Descent (BGD)**, **Stochastic Gradient Descent (SGD)**, and **Mini-Batch Gradient Descent**. These optimization methods are foundational in machine learning and are used to minimize functions like the loss function in regression or neural networks.

The code is in the **`grad_types.py`** file and provides:
- Implementations of each gradient descent method.
- A comparison of their performance using synthetic data.
- Visualization of loss curves and predictions to understand their behavior.

---